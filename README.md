


### Resume description
1. Developed a Transformer-based Neural Machine Translation (NMT) model from scratch, implementing core architectural components including Multi-Head Attention, Positional Encodings, Layer Normalization, and Feed-Forward Networks.
2. Engineered a complete Encoder-Decoder architecture, handling masked self-attention in the decoder to prevent attending to future tokens, enabling sequential generation of translations.
3. Implemented forward and backward propagation steps for the Transformer model, optimizing word and context embedding matrices over 200 epochs on a subset of the Multi30k dataset.
4. Achieved significant training progress with a final loss of approximately 0.15 (assuming this was your approximate final loss) by the end of training.
5. Demonstrated machine translation capabilities using a greedy decoding strategy, successfully translating unseen English sentences to German and qualitatively assessing the accuracy against reference translations.







