{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "# put your student name here, you will need to train the model that prints out your name in each loss\n",
        "# without the name, you will not be able to get points in train part\n",
        "STUDENT_NAME = \"Rohan Tikotekar *_* \""
      ],
      "metadata": {
        "id": "jbYfCX1-csCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary\n",
        "Credit: https://nlp.seas.harvard.edu/annotated-transformer/\n",
        "\n",
        "Download required packages, Imports libraries, sets random seeds."
      ],
      "metadata": {
        "id": "nwdhro5z9J_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torchaudio torchvision\n",
        "!pip install --force-reinstall torchtext==0.16.2\n",
        "!pip install portalocker>=2.0.0\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install numpy==1.26\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c2DRfGV-nyG4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5af82e8d-5311-449b-c8c0-8d84ff4d8938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torchtext==0.16.2\n",
            "  Using cached torchtext-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting tqdm (from torchtext==0.16.2)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting requests (from torchtext==0.16.2)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting torch==2.1.2 (from torchtext==0.16.2)\n",
            "  Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting numpy (from torchtext==0.16.2)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting torchdata==0.7.1 (from torchtext==0.16.2)\n",
            "  Using cached torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting filelock (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting urllib3>=1.25 (from torchdata==0.7.1->torchtext==0.16.2)\n",
            "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.9.41-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->torchtext==0.16.2)\n",
            "  Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torchtext==0.16.2)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torchtext==0.16.2)\n",
            "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.1.2->torchtext==0.16.2)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Using cached torchtext-0.16.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "Using cached torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.9.41-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchdata, torchtext\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
            "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.0\n",
            "    Uninstalling numpy-1.26.0:\n",
            "      Successfully uninstalled numpy-1.26.0\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.2\n",
            "    Uninstalling charset-normalizer-3.4.2:\n",
            "      Successfully uninstalled charset-normalizer-3.4.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.2\n",
            "    Uninstalling torch-2.1.2:\n",
            "      Successfully uninstalled torch-2.1.2\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.7.1\n",
            "    Uninstalling torchdata-0.7.1:\n",
            "      Successfully uninstalled torchdata-0.7.1\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.2\n",
            "    Uninstalling torchtext-0.16.2:\n",
            "      Successfully uninstalled torchtext-0.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.15 requires torchvision, which is not installed.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 certifi-2025.4.26 charset-normalizer-3.4.2 filelock-3.18.0 fsspec-2025.3.2 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.9.41 nvidia-nvtx-cu12-12.1.105 requests-2.32.3 sympy-1.14.0 torch-2.1.2 torchdata-0.7.1 torchtext-0.16.2 tqdm-4.67.1 triton-2.1.0 typing-extensions-4.13.2 urllib3-2.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "markupsafe",
                  "mpmath",
                  "requests",
                  "sympy",
                  "torch",
                  "torchdata",
                  "torchgen",
                  "torchtext",
                  "tqdm"
                ]
              },
              "id": "3970e11f4046450b9795fbbf933c3cb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/compat.py\", line 4, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/types.py\", line 27, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/compat.py\", line 35, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/compat.py\", line 4, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/types.py\", line 27, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/compat.py\", line 35, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting numpy==1.26\n",
            "  Using cached numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Using cached numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "import spacy\n",
        "import warnings\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Set to False to skip notebook execution (e.g. for debugging)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ],
      "metadata": {
        "id": "19_RLM_39Vk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Model Architecture"
      ],
      "metadata": {
        "id": "GGPNygrg28AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Embedding (5pt)\n",
        "\n",
        "Defines the word-embedding layer that maps token indices to dense vectors.\n",
        "\n",
        "The shape of the Embedding should be: **[Vocab, Embedding_size]**"
      ],
      "metadata": {
        "id": "av_WHKp6CayD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        ######### add your code here\n",
        "        self.emb = nn.Embedding(vocab, d_model)\n",
        "        #########\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "3Ikf1cvYCbVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Position Encoding (15pt)\n",
        "\n",
        "Implements sinusoidal positional encodings so the model can attend to token order without recurrence.\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\\\\n",
        "PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\n",
        "$$\n",
        "\n",
        "1. `pos` is the position `i` is the dimension.\n",
        "2. Each dimension of the position encoding corresponds to a sinusoid.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LCqKUlcZCfOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        ######### add your code here\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        #########\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "EuTdaT08CfUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 LayerNorm (10pt)\n",
        "Provides a custom Layer Normalization module to stabilise training by normalising hidden states across features.\n",
        "\n",
        "We will follow the basic setting in [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
        "\n",
        "$$\n",
        "y = \\frac{x - E[x]}{Var[x] + \\epsilon}*\\gamma + \\beta\n",
        "$$\n",
        "\n",
        "where:\n",
        "1. `ϵ`: a value added to the denominator for numerical stability\n",
        "2. `γ` & `β`: weight (initialized to 1) and bias (initalized to 0)."
      ],
      "metadata": {
        "id": "lfornq-8_jrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module.\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        ######### add your code here\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        #########\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        ######### add your code here\n",
        "        result = self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "        #########\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "Pukl4c0j_njP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 SubLayer Connection (10pt)\n",
        "\n",
        "Wraps a residual connection around a sub-layer (e.g., attention or FFN). Right here, we apply the pre-norm setup.\n",
        "\n",
        "The output of each sub-layer is:\n",
        "$$\n",
        "LayerNorm(x + Sublayer(x))\n",
        "$$\n",
        "where:\n",
        "1. `Sublayer(x)` is the function implemented by the sub-layer itself.(Attn/MLP)"
      ],
      "metadata": {
        "id": "pUoz2hQ6AG9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        ######### add your code here\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "        #########\n"
      ],
      "metadata": {
        "id": "9jg9mHK9AHVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Multi-Head Attention (20pt)\n",
        "\n",
        "Implements scaled dot-product self-attention, splits it into h parallel “heads,” and concatenates the results back together.\n",
        "\n",
        "For each head:\n",
        "$$\n",
        "Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "Multi-Head Attention:\n",
        "$$\n",
        "MultiHead(Q, K, V) = Concat(head_1,\\dots, head_h)W^O \\\\\n",
        "where\\quad head_i=Attention(QW^Q_i, KW^K_i, VW^V_i)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "LtUUG8X9A3xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    ######### add your code here\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "    #########\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    ######### add your code here\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    #########\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n"
      ],
      "metadata": {
        "id": "8sVsO2_xAtLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.Wq = nn.Linear(d_model, d_model)\n",
        "        self.Wk = nn.Linear(d_model, d_model)\n",
        "        self.Wv = nn.Linear(d_model, d_model)\n",
        "        self.linears = nn.Linear(d_model, d_model)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        ######### add your code here\n",
        "        query = self.Wq(query).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key = self.Wk(key).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        value = self.Wv(value).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        #########\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(\n",
        "            query, key, value, mask=mask, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        ######### add your code here\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        #########\n",
        "\n",
        "        del query\n",
        "        del key\n",
        "        del value\n",
        "        return self.linears(x)\n"
      ],
      "metadata": {
        "id": "KKXVrr7EAv3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 FeedFordward Network (FFN) (5pt)\n",
        "Creates the two-layer position-wise feed-forward network applied after each attention block."
      ],
      "metadata": {
        "id": "yL0oZM8eA-LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        ######### add your code here\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        #########\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.w_1(x).relu()))\n"
      ],
      "metadata": {
        "id": "_Z_Njzq4A-eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Encoder"
      ],
      "metadata": {
        "id": "QOWx1p27-wrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Encoder Layer\n",
        "\n",
        "Defines a single Encoder block consisting of a Multi-Head Self-Attention layer and an FFN, each wrapped in Add & Norm."
      ],
      "metadata": {
        "id": "SVODKZfa_7rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "wJWBzFyt-64U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Encoder Stacks (10pt)\n",
        "Stacks N Encoder blocks to form the full Encoder."
      ],
      "metadata": {
        "id": "eLfbujxM_411"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module.\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        ######### add your code here\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        #########\n",
        "        return self.norm(x)\n"
      ],
      "metadata": {
        "id": "Zp6UtcVC-0El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Decoder"
      ],
      "metadata": {
        "id": "A-yz1UvNAaD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Decoder Layer (5pt)\n",
        "\n",
        "Details a single Decoder block containing masked self-attention, cross-attention, and an FFN, each with residual Add & Norm."
      ],
      "metadata": {
        "id": "KxTvioDZAibw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module.\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        ######### add your code here\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        #########\n",
        "        return self.sublayer[2](x, self.feed_forward)\n"
      ],
      "metadata": {
        "id": "EgWip7SNAkqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Decoder Stacks (10pt)\n",
        "Stacks N Decoder blocks to build the complete Decoder."
      ],
      "metadata": {
        "id": "naHuN-v8Ae8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module.\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        ######### add your code here\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        #########\n",
        "        return self.norm(x)\n"
      ],
      "metadata": {
        "id": "abuJsIP5AZbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Combines the Encoder and Decoder into a full Transformer model and adds a linear projection to produce logits over the target vocabulary."
      ],
      "metadata": {
        "id": "gQmMX48iCn0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many\n",
        "    other models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "CAlGGs3DCvyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(\n",
        "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
        "):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab),\n",
        "    )\n",
        "\n",
        "    # This was important from their code.\n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "metadata": {
        "id": "gn3geYnbCpWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train (10pt)\n",
        "\n",
        "Prepares a small EN→DE dataset, builds dataloaders, defines the loss, optimizer, and training loop, then runs a quick demonstration training epoch followed by an inference example."
      ],
      "metadata": {
        "id": "qFaaJLjmErAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import spacy\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import copy\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module.\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = scores.softmax(dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        query, key, value = [\n",
        "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "            for l, x in zip(self.linears, (query, key, value))\n",
        "        ]\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(\n",
        "            query, key, value, mask=mask, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many\n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.functional.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "def subsequent_mask(size, device=None):\n",
        "    # Mask has 1s in the *allowed* (≤ i) positions, 0 elsewhere\n",
        "    attn_shape = (1, size, size)\n",
        "    mask = torch.triu(torch.ones(attn_shape, dtype=torch.bool, device=device), diagonal=1)\n",
        "    return ~mask     # invert so future positions are 0\n",
        "\n",
        "# Load tokenizers (ensure you've downloaded the models:\n",
        "# python -m spacy download en_core_web_sm && python -m spacy download de_core_news_sm)\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Prepare dataset (only 500 samples for a quick test)\n",
        "raw_iter = Multi30k(split='train', language_pair=('en', 'de'))\n",
        "raw_data = list(raw_iter)[:500]\n",
        "test_iter = Multi30k(split='valid', language_pair=('en', 'de'))\n",
        "test_data = list(test_iter)[:10]\n",
        "\n",
        "# Build vocabularies with special tokens\n",
        "SRC_SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "TGT_SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocab_src = build_vocab_from_iterator((tokenize_en(pair[0]) for pair in raw_data), specials=SRC_SPECIALS)\n",
        "vocab_tgt = build_vocab_from_iterator((tokenize_de(pair[1]) for pair in raw_data), specials=TGT_SPECIALS)\n",
        "\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])\n",
        "\n",
        "SRC_PAD_IDX = vocab_src['<pad>']\n",
        "TGT_PAD_IDX = vocab_tgt['<pad>']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src, tgt in batch:\n",
        "        src_tokens = [vocab_src['<bos>']] + [vocab_src[token] for token in tokenize_en(src)] + [vocab_src['<eos>']]\n",
        "        tgt_tokens = [vocab_tgt['<bos>']] + [vocab_tgt[token] for token in tokenize_de(tgt)] + [vocab_tgt['<eos>']]\n",
        "        src_batch.append(torch.tensor(src_tokens, dtype=torch.long))\n",
        "        tgt_batch.append(torch.tensor(tgt_tokens, dtype=torch.long))\n",
        "    src_batch = pad_sequence(src_batch, padding_value=SRC_PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=TGT_PAD_IDX, batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "train_loader = DataLoader(raw_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = make_model(len(vocab_src), len(vocab_tgt), N=2, d_model=64, d_ff=128, h=4, dropout=0.1).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.NLLLoss(ignore_index=TGT_PAD_IDX)\n",
        "\n",
        "STUDENT_NAME = \"Rohan Tikotekar *_*\"  #  Add the student name.\n",
        "\n",
        "model.train()\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    total_loss = 0\n",
        "    for i, (src, tgt) in enumerate(train_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        src_mask = (src != SRC_PAD_IDX).unsqueeze(-2)\n",
        "        tgt_pad_mask = (tgt_input != TGT_PAD_IDX).unsqueeze(-2)\n",
        "        tgt_mask    = tgt_pad_mask & subsequent_mask(tgt_input.size(1), device=tgt_input.device)\n",
        "        ######### add your code here\n",
        "        out = model(src, tgt_input, src_mask, tgt_mask)\n",
        "        #########\n",
        "        logits = model.generator(out)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"{STUDENT_NAME} Epoch {epoch} | Loss: {total_loss/(i+1):.4f} | Time: {time.time()-start_time:.2f}s\")\n"
      ],
      "metadata": {
        "id": "_SU86-VkEuOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccc3c17-4463-42af-98fd-0bf82b6604ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rohan Tikotekar *_* Epoch 0 | Loss: 7.1798 | Time: 2.62s\n",
            "Rohan Tikotekar *_* Epoch 1 | Loss: 6.9746 | Time: 1.88s\n",
            "Rohan Tikotekar *_* Epoch 2 | Loss: 6.8473 | Time: 1.88s\n",
            "Rohan Tikotekar *_* Epoch 3 | Loss: 6.7434 | Time: 1.92s\n",
            "Rohan Tikotekar *_* Epoch 4 | Loss: 6.6464 | Time: 2.53s\n",
            "Rohan Tikotekar *_* Epoch 5 | Loss: 6.5495 | Time: 2.63s\n",
            "Rohan Tikotekar *_* Epoch 6 | Loss: 6.4606 | Time: 1.96s\n",
            "Rohan Tikotekar *_* Epoch 7 | Loss: 6.3729 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 8 | Loss: 6.2834 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 9 | Loss: 6.1953 | Time: 1.89s\n",
            "Rohan Tikotekar *_* Epoch 10 | Loss: 6.1131 | Time: 2.57s\n",
            "Rohan Tikotekar *_* Epoch 11 | Loss: 6.0348 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 12 | Loss: 5.9531 | Time: 1.77s\n",
            "Rohan Tikotekar *_* Epoch 13 | Loss: 5.8798 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 14 | Loss: 5.8091 | Time: 1.81s\n",
            "Rohan Tikotekar *_* Epoch 15 | Loss: 5.7398 | Time: 1.80s\n",
            "Rohan Tikotekar *_* Epoch 16 | Loss: 5.6784 | Time: 2.14s\n",
            "Rohan Tikotekar *_* Epoch 17 | Loss: 5.6138 | Time: 2.34s\n",
            "Rohan Tikotekar *_* Epoch 18 | Loss: 5.5546 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 19 | Loss: 5.5039 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 20 | Loss: 5.4561 | Time: 1.87s\n",
            "Rohan Tikotekar *_* Epoch 21 | Loss: 5.4077 | Time: 2.93s\n",
            "Rohan Tikotekar *_* Epoch 22 | Loss: 5.3606 | Time: 2.19s\n",
            "Rohan Tikotekar *_* Epoch 23 | Loss: 5.3272 | Time: 2.21s\n",
            "Rohan Tikotekar *_* Epoch 24 | Loss: 5.2825 | Time: 1.82s\n",
            "Rohan Tikotekar *_* Epoch 25 | Loss: 5.2484 | Time: 1.81s\n",
            "Rohan Tikotekar *_* Epoch 26 | Loss: 5.2073 | Time: 1.89s\n",
            "Rohan Tikotekar *_* Epoch 27 | Loss: 5.1771 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 28 | Loss: 5.1420 | Time: 1.87s\n",
            "Rohan Tikotekar *_* Epoch 29 | Loss: 5.1099 | Time: 2.60s\n",
            "Rohan Tikotekar *_* Epoch 30 | Loss: 5.0782 | Time: 1.81s\n",
            "Rohan Tikotekar *_* Epoch 31 | Loss: 5.0441 | Time: 1.76s\n",
            "Rohan Tikotekar *_* Epoch 32 | Loss: 5.0129 | Time: 1.75s\n",
            "Rohan Tikotekar *_* Epoch 33 | Loss: 4.9804 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 34 | Loss: 4.9420 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 35 | Loss: 4.9020 | Time: 2.12s\n",
            "Rohan Tikotekar *_* Epoch 36 | Loss: 4.8631 | Time: 2.36s\n",
            "Rohan Tikotekar *_* Epoch 37 | Loss: 4.8205 | Time: 1.81s\n",
            "Rohan Tikotekar *_* Epoch 38 | Loss: 4.7933 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 39 | Loss: 4.7466 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 40 | Loss: 4.7106 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 41 | Loss: 4.6769 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 42 | Loss: 4.6332 | Time: 2.42s\n",
            "Rohan Tikotekar *_* Epoch 43 | Loss: 4.5996 | Time: 2.07s\n",
            "Rohan Tikotekar *_* Epoch 44 | Loss: 4.5675 | Time: 1.78s\n",
            "Rohan Tikotekar *_* Epoch 45 | Loss: 4.5351 | Time: 1.76s\n",
            "Rohan Tikotekar *_* Epoch 46 | Loss: 4.5028 | Time: 1.77s\n",
            "Rohan Tikotekar *_* Epoch 47 | Loss: 4.4671 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 48 | Loss: 4.4320 | Time: 2.02s\n",
            "Rohan Tikotekar *_* Epoch 49 | Loss: 4.4018 | Time: 2.48s\n",
            "Rohan Tikotekar *_* Epoch 50 | Loss: 4.3754 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 51 | Loss: 4.3415 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 52 | Loss: 4.3112 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 53 | Loss: 4.2807 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 54 | Loss: 4.2565 | Time: 1.75s\n",
            "Rohan Tikotekar *_* Epoch 55 | Loss: 4.2291 | Time: 2.25s\n",
            "Rohan Tikotekar *_* Epoch 56 | Loss: 4.2061 | Time: 2.14s\n",
            "Rohan Tikotekar *_* Epoch 57 | Loss: 4.1762 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 58 | Loss: 4.1483 | Time: 1.75s\n",
            "Rohan Tikotekar *_* Epoch 59 | Loss: 4.1211 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 60 | Loss: 4.1047 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 61 | Loss: 4.0772 | Time: 1.97s\n",
            "Rohan Tikotekar *_* Epoch 62 | Loss: 4.0479 | Time: 2.55s\n",
            "Rohan Tikotekar *_* Epoch 63 | Loss: 4.0364 | Time: 1.88s\n",
            "Rohan Tikotekar *_* Epoch 64 | Loss: 4.0120 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 65 | Loss: 3.9872 | Time: 1.78s\n",
            "Rohan Tikotekar *_* Epoch 66 | Loss: 3.9591 | Time: 1.87s\n",
            "Rohan Tikotekar *_* Epoch 67 | Loss: 3.9503 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 68 | Loss: 3.9283 | Time: 2.28s\n",
            "Rohan Tikotekar *_* Epoch 69 | Loss: 3.9038 | Time: 2.18s\n",
            "Rohan Tikotekar *_* Epoch 70 | Loss: 3.8820 | Time: 1.88s\n",
            "Rohan Tikotekar *_* Epoch 71 | Loss: 3.8629 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 72 | Loss: 3.8470 | Time: 1.82s\n",
            "Rohan Tikotekar *_* Epoch 73 | Loss: 3.8288 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 74 | Loss: 3.8026 | Time: 1.82s\n",
            "Rohan Tikotekar *_* Epoch 75 | Loss: 3.7860 | Time: 2.56s\n",
            "Rohan Tikotekar *_* Epoch 76 | Loss: 3.7725 | Time: 1.87s\n",
            "Rohan Tikotekar *_* Epoch 77 | Loss: 3.7498 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 78 | Loss: 3.7377 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 79 | Loss: 3.7237 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 80 | Loss: 3.7028 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 81 | Loss: 3.6863 | Time: 2.21s\n",
            "Rohan Tikotekar *_* Epoch 82 | Loss: 3.6665 | Time: 2.22s\n",
            "Rohan Tikotekar *_* Epoch 83 | Loss: 3.6561 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 84 | Loss: 3.6341 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 85 | Loss: 3.6220 | Time: 1.75s\n",
            "Rohan Tikotekar *_* Epoch 86 | Loss: 3.6058 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 87 | Loss: 3.5871 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 88 | Loss: 3.5753 | Time: 2.59s\n",
            "Rohan Tikotekar *_* Epoch 89 | Loss: 3.5524 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 90 | Loss: 3.5522 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 91 | Loss: 3.5342 | Time: 3.38s\n",
            "Rohan Tikotekar *_* Epoch 92 | Loss: 3.5202 | Time: 3.15s\n",
            "Rohan Tikotekar *_* Epoch 93 | Loss: 3.4939 | Time: 2.51s\n",
            "Rohan Tikotekar *_* Epoch 94 | Loss: 3.4839 | Time: 1.93s\n",
            "Rohan Tikotekar *_* Epoch 95 | Loss: 3.4742 | Time: 1.89s\n",
            "Rohan Tikotekar *_* Epoch 96 | Loss: 3.4532 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 97 | Loss: 3.4361 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 98 | Loss: 3.4194 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 99 | Loss: 3.4182 | Time: 2.22s\n",
            "Rohan Tikotekar *_* Epoch 100 | Loss: 3.4040 | Time: 2.26s\n",
            "Rohan Tikotekar *_* Epoch 101 | Loss: 3.3896 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 102 | Loss: 3.3700 | Time: 1.91s\n",
            "Rohan Tikotekar *_* Epoch 103 | Loss: 3.3666 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 104 | Loss: 3.3496 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 105 | Loss: 3.3294 | Time: 1.94s\n",
            "Rohan Tikotekar *_* Epoch 106 | Loss: 3.3214 | Time: 2.49s\n",
            "Rohan Tikotekar *_* Epoch 107 | Loss: 3.3046 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 108 | Loss: 3.2961 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 109 | Loss: 3.2833 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 110 | Loss: 3.2570 | Time: 1.90s\n",
            "Rohan Tikotekar *_* Epoch 111 | Loss: 3.2474 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 112 | Loss: 3.2411 | Time: 2.27s\n",
            "Rohan Tikotekar *_* Epoch 113 | Loss: 3.2244 | Time: 2.21s\n",
            "Rohan Tikotekar *_* Epoch 114 | Loss: 3.2162 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 115 | Loss: 3.1947 | Time: 1.81s\n",
            "Rohan Tikotekar *_* Epoch 116 | Loss: 3.1820 | Time: 1.82s\n",
            "Rohan Tikotekar *_* Epoch 117 | Loss: 3.1666 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 118 | Loss: 3.1538 | Time: 1.93s\n",
            "Rohan Tikotekar *_* Epoch 119 | Loss: 3.1450 | Time: 2.64s\n",
            "Rohan Tikotekar *_* Epoch 120 | Loss: 3.1394 | Time: 1.80s\n",
            "Rohan Tikotekar *_* Epoch 121 | Loss: 3.1107 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 122 | Loss: 3.1044 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 123 | Loss: 3.1020 | Time: 1.87s\n",
            "Rohan Tikotekar *_* Epoch 124 | Loss: 3.0739 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 125 | Loss: 3.0601 | Time: 2.28s\n",
            "Rohan Tikotekar *_* Epoch 126 | Loss: 3.0556 | Time: 2.23s\n",
            "Rohan Tikotekar *_* Epoch 127 | Loss: 3.0490 | Time: 1.80s\n",
            "Rohan Tikotekar *_* Epoch 128 | Loss: 3.0347 | Time: 1.89s\n",
            "Rohan Tikotekar *_* Epoch 129 | Loss: 3.0263 | Time: 1.91s\n",
            "Rohan Tikotekar *_* Epoch 130 | Loss: 2.9976 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 131 | Loss: 2.9984 | Time: 2.11s\n",
            "Rohan Tikotekar *_* Epoch 132 | Loss: 2.9791 | Time: 2.49s\n",
            "Rohan Tikotekar *_* Epoch 133 | Loss: 2.9645 | Time: 1.88s\n",
            "Rohan Tikotekar *_* Epoch 134 | Loss: 2.9505 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 135 | Loss: 2.9474 | Time: 1.90s\n",
            "Rohan Tikotekar *_* Epoch 136 | Loss: 2.9276 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 137 | Loss: 2.9087 | Time: 1.87s\n",
            "Rohan Tikotekar *_* Epoch 138 | Loss: 2.9126 | Time: 2.62s\n",
            "Rohan Tikotekar *_* Epoch 139 | Loss: 2.9013 | Time: 1.98s\n",
            "Rohan Tikotekar *_* Epoch 140 | Loss: 2.8831 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 141 | Loss: 2.8756 | Time: 1.93s\n",
            "Rohan Tikotekar *_* Epoch 142 | Loss: 2.8638 | Time: 1.89s\n",
            "Rohan Tikotekar *_* Epoch 143 | Loss: 2.8444 | Time: 1.91s\n",
            "Rohan Tikotekar *_* Epoch 144 | Loss: 2.8280 | Time: 2.32s\n",
            "Rohan Tikotekar *_* Epoch 145 | Loss: 2.8189 | Time: 2.29s\n",
            "Rohan Tikotekar *_* Epoch 146 | Loss: 2.7993 | Time: 1.91s\n",
            "Rohan Tikotekar *_* Epoch 147 | Loss: 2.7935 | Time: 1.95s\n",
            "Rohan Tikotekar *_* Epoch 148 | Loss: 2.7878 | Time: 1.92s\n",
            "Rohan Tikotekar *_* Epoch 149 | Loss: 2.7700 | Time: 1.90s\n",
            "Rohan Tikotekar *_* Epoch 150 | Loss: 2.7590 | Time: 2.12s\n",
            "Rohan Tikotekar *_* Epoch 151 | Loss: 2.7504 | Time: 2.43s\n",
            "Rohan Tikotekar *_* Epoch 152 | Loss: 2.7343 | Time: 1.92s\n",
            "Rohan Tikotekar *_* Epoch 153 | Loss: 2.7209 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 154 | Loss: 2.7237 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 155 | Loss: 2.6922 | Time: 1.90s\n",
            "Rohan Tikotekar *_* Epoch 156 | Loss: 2.6854 | Time: 1.91s\n",
            "Rohan Tikotekar *_* Epoch 157 | Loss: 2.6740 | Time: 2.69s\n",
            "Rohan Tikotekar *_* Epoch 158 | Loss: 2.6658 | Time: 1.94s\n",
            "Rohan Tikotekar *_* Epoch 159 | Loss: 2.6546 | Time: 1.89s\n",
            "Rohan Tikotekar *_* Epoch 160 | Loss: 2.6397 | Time: 1.91s\n",
            "Rohan Tikotekar *_* Epoch 161 | Loss: 2.6185 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 162 | Loss: 2.6142 | Time: 1.95s\n",
            "Rohan Tikotekar *_* Epoch 163 | Loss: 2.5980 | Time: 2.37s\n",
            "Rohan Tikotekar *_* Epoch 164 | Loss: 2.5973 | Time: 2.23s\n",
            "Rohan Tikotekar *_* Epoch 165 | Loss: 2.5785 | Time: 1.91s\n",
            "Rohan Tikotekar *_* Epoch 166 | Loss: 2.5646 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 167 | Loss: 2.5588 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 168 | Loss: 2.5437 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 169 | Loss: 2.5288 | Time: 2.07s\n",
            "Rohan Tikotekar *_* Epoch 170 | Loss: 2.5218 | Time: 2.46s\n",
            "Rohan Tikotekar *_* Epoch 171 | Loss: 2.5116 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 172 | Loss: 2.5048 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 173 | Loss: 2.4908 | Time: 1.88s\n",
            "Rohan Tikotekar *_* Epoch 174 | Loss: 2.4760 | Time: 1.82s\n",
            "Rohan Tikotekar *_* Epoch 175 | Loss: 2.4693 | Time: 2.47s\n",
            "Rohan Tikotekar *_* Epoch 176 | Loss: 2.4577 | Time: 2.92s\n",
            "Rohan Tikotekar *_* Epoch 177 | Loss: 2.4466 | Time: 2.03s\n",
            "Rohan Tikotekar *_* Epoch 178 | Loss: 2.4332 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 179 | Loss: 2.4224 | Time: 1.82s\n",
            "Rohan Tikotekar *_* Epoch 180 | Loss: 2.4160 | Time: 1.82s\n",
            "Rohan Tikotekar *_* Epoch 181 | Loss: 2.3995 | Time: 1.85s\n",
            "Rohan Tikotekar *_* Epoch 182 | Loss: 2.3919 | Time: 2.19s\n",
            "Rohan Tikotekar *_* Epoch 183 | Loss: 2.3771 | Time: 2.36s\n",
            "Rohan Tikotekar *_* Epoch 184 | Loss: 2.3717 | Time: 1.79s\n",
            "Rohan Tikotekar *_* Epoch 185 | Loss: 2.3572 | Time: 1.90s\n",
            "Rohan Tikotekar *_* Epoch 186 | Loss: 2.3521 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 187 | Loss: 2.3440 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 188 | Loss: 2.3288 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 189 | Loss: 2.3120 | Time: 2.62s\n",
            "Rohan Tikotekar *_* Epoch 190 | Loss: 2.3045 | Time: 1.94s\n",
            "Rohan Tikotekar *_* Epoch 191 | Loss: 2.2992 | Time: 1.78s\n",
            "Rohan Tikotekar *_* Epoch 192 | Loss: 2.2870 | Time: 1.86s\n",
            "Rohan Tikotekar *_* Epoch 193 | Loss: 2.2828 | Time: 1.84s\n",
            "Rohan Tikotekar *_* Epoch 194 | Loss: 2.2669 | Time: 1.81s\n",
            "Rohan Tikotekar *_* Epoch 195 | Loss: 2.2548 | Time: 2.17s\n",
            "Rohan Tikotekar *_* Epoch 196 | Loss: 2.2474 | Time: 2.27s\n",
            "Rohan Tikotekar *_* Epoch 197 | Loss: 2.2256 | Time: 1.82s\n",
            "Rohan Tikotekar *_* Epoch 198 | Loss: 2.2150 | Time: 1.83s\n",
            "Rohan Tikotekar *_* Epoch 199 | Loss: 2.2115 | Time: 1.85s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "2IYPYTNoa0KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding for inference\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1, dtype=torch.long).fill_(start_symbol).to(device)\n",
        "    for _ in range(max_len - 1):\n",
        "        tgt_mask = subsequent_mask(ys.size(1)).to(device)\n",
        "        out = model.decode(memory, src_mask, ys, tgt_mask)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        next_word = torch.argmax(prob, dim=1).item()\n",
        "        ys = torch.cat([ys, torch.tensor([[next_word]], dtype=torch.long).to(device)], dim=1)\n",
        "        if next_word == vocab_tgt['<eos>']:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "model.eval()\n",
        "for index, example in enumerate(test_data[:10]):\n",
        "    src = torch.tensor(\n",
        "        [vocab_src['<bos>']] + [vocab_src[t] for t in tokenize_en(example[0])] + [vocab_src['<eos>']],\n",
        "        dtype=torch.long\n",
        "    ).unsqueeze(0).to(device)\n",
        "    src_mask = (src != SRC_PAD_IDX).unsqueeze(-2)\n",
        "    translation = greedy_decode(model, src, src_mask, max_len=50, start_symbol=vocab_tgt['<bos>'])\n",
        "    tokens = [vocab_tgt.get_itos()[idx] for idx in translation.squeeze().tolist()]\n",
        "\n",
        "    print(\"Source:\", example[0])\n",
        "    print(\"Reference:\", example[1])\n",
        "    print(\"Predicted:\", \" \".join(tokens))\n",
        "    print('*******'*50)"
      ],
      "metadata": {
        "id": "4eXzi_Kaa0Uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05116cc1-5f8e-475f-a8b3-08f0eb6c3eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: A group of men are loading cotton onto a truck\n",
            "Reference: Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
            "Predicted: <bos> eine gruppe von eine person in der nähe eines großen . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: A man sleeping in a green room on a couch.\n",
            "Reference: Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
            "Predicted: <bos> ein mann in einem schwarzen hemd und ein mann mit einer bank . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: A boy wearing headphones sits on a woman's shoulders.\n",
            "Reference: Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
            "Predicted: <bos> ein junge auf einer straße auf einem ball . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: Two men setting up a blue ice fishing hut on an iced over lake\n",
            "Reference: Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\n",
            "Predicted: <bos> zwei männer , die in einem roten oberteil und eine bahn . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: A balding man wearing a red life jacket is sitting in a small boat.\n",
            "Reference: Ein Mann mit beginnender Glatze, der eine rote Rettungsweste trägt, sitzt in einem kleinen Boot.\n",
            "Predicted: <bos> ein mann in einem roten hemd sitzt auf einem roten hosen , der nähe eines großen . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: A lady in a red coat, holding a bluish hand bag likely of asian descent, jumping off the ground for a snapshot.\n",
            "Reference: Eine Frau in einem rotem Mantel, die eine vermutlich aus Asien stammende Handtasche in einem blauen Farbton hält, springt für einen Schnappschuss in die Luft.\n",
            "Predicted: <bos> eine frau in einem roten hemd und eine frau , mit einem roten oberteil , mit einem großen weihnachtslichter . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: A brown dog is running after the black dog.\n",
            "Reference: Ein brauner Hund rennt dem schwarzen Hund hinterher.\n",
            "Predicted: <bos> ein hund rennt rennt am strand . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch.\n",
            "Reference: Ein kleiner Junge mit einem Giants-Trikot schwingt einen Baseballschläger in Richtung eines ankommenden Balls.\n",
            "Predicted: <bos> ein junge in einem roten hemd und ein mann in einem roten hosen , das zu . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: A man in a cluttered office is using the telephone\n",
            "Reference: Ein Mann telefoniert in einem unaufgeräumten Büro\n",
            "Predicted: <bos> ein mann in einem roten hemd trägt ein mann . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
            "Source: A smiling woman in a peach tank top stands holding a mountain bike\n",
            "Reference: Eine lächelnde Frau mit einem pfirsichfarbenen Trägershirt hält ein Mountainbike\n",
            "Predicted: <bos> eine frau in einem roten hemd und einen blauen trägt und ein anderer mann steht . <eos>\n",
            "**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n"
          ]
        }
      ]
    }
  ]
}